{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ep_statistical_models",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_aU9e5WJwaW",
        "colab_type": "text"
      },
      "source": [
        "Part 2 of **time series forecasting with energy**\n",
        "\n",
        "In this section, I will investigate the problem at hand, formalising it, outlining methods for interpreting error, and setup a baseline using standard statistical methods for time-series analysis. The library `statsmodels` will come in handy with this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haBUkdRBIziF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LOAD THE REPOSITORY\n",
        "# if you are working from outside the repository\n",
        "# this happens if you use colab like I do\n",
        "!git clone https://github.com/sandeshbhatjr/energy-prediction.git\n",
        "!pip install -U --quiet pandas statsmodels tables numpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRXzaf6uM2NU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime as dt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.read_hdf('energy-prediction/data/clean_german_df', key='df_with_load_and_gen')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-Ef_MEHTfQI",
        "colab_type": "text"
      },
      "source": [
        "If the read_hdf throws an error, then it's probably because the version of pyTables, or numpy is the wrong one. This is a known bug in some combination of versions of those libraries: see https://stackoverflow.com/questions/54210073/pd-read-hdf-throws-cannot-set-writable-flag-to-true-of-this-array for details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hN3GD0GIe9K",
        "colab_type": "text"
      },
      "source": [
        "# Statistical models\n",
        "\n",
        "This part will be technical. The purpose is to explore SOTA models for time-series forecasting, and their applicability to day-ahead price forecasting. In a typical use-case, we have access to a history of recorded day-ahead prices, and we are interested in forecasting the day-ahead prices in an interval consisting of a tuple of 24 prices for each day. The problem can be formulated precisely as follows.\n",
        ">  Let $\\{t_1, t_2, t_3, ..., t_k \\} := \\mathfrak{T}$ represent a collection of dates in  chronological order, $p_t \\in \\mathbb{R}^{24}$ be the day-ahead price indexed by $t \\in \\mathfrak{T}$. Given some historic day-ahead prices $p_{t_1}, p_{t_2}, ... ,p_{t_k}$, forecast the future day-ahead prices $p_{t_{k+1}}, ..., p_{t_{k+h}}$ in a certain future window-size of $h \\in \\mathbb{N}$. In addition, we have a collection of exogenous time series- $d^{\\alpha}_{t_1},d^{\\alpha}_{t_2}, ..., d^{\\alpha}_{t_k}$ indexed by $\\alpha$, which includes the generation and consumption forecasts. Finally for each $t\\in {t_1, ..., t_{k+h'}}, h'>> h$, we have some associated features $f^{\\beta}_t$, where the features are known well into future (for example, if the day is a holiday or not)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8j275Z-IjbF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "outputId": "fea5dd51-172d-475b-b5bb-e7a58e872fc7"
      },
      "source": [
        "def convert_to_multivariate_series(df, data_columns):\n",
        "  return df[data_columns].groupby(by=[df.index.date, df.index.time]).mean().unstack(-1)\n",
        "\n",
        "convert_to_multivariate_series(df, ['Day Ahead Price', 'Wind offshore Generation'])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"24\" halign=\"left\">Day Ahead Price</th>\n",
              "      <th colspan=\"24\" halign=\"left\">Wind offshore Generation</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>00:00:00</th>\n",
              "      <th>01:00:00</th>\n",
              "      <th>02:00:00</th>\n",
              "      <th>03:00:00</th>\n",
              "      <th>04:00:00</th>\n",
              "      <th>05:00:00</th>\n",
              "      <th>06:00:00</th>\n",
              "      <th>07:00:00</th>\n",
              "      <th>08:00:00</th>\n",
              "      <th>09:00:00</th>\n",
              "      <th>10:00:00</th>\n",
              "      <th>11:00:00</th>\n",
              "      <th>12:00:00</th>\n",
              "      <th>13:00:00</th>\n",
              "      <th>14:00:00</th>\n",
              "      <th>15:00:00</th>\n",
              "      <th>16:00:00</th>\n",
              "      <th>17:00:00</th>\n",
              "      <th>18:00:00</th>\n",
              "      <th>19:00:00</th>\n",
              "      <th>20:00:00</th>\n",
              "      <th>21:00:00</th>\n",
              "      <th>22:00:00</th>\n",
              "      <th>23:00:00</th>\n",
              "      <th>00:00:00</th>\n",
              "      <th>01:00:00</th>\n",
              "      <th>02:00:00</th>\n",
              "      <th>03:00:00</th>\n",
              "      <th>04:00:00</th>\n",
              "      <th>05:00:00</th>\n",
              "      <th>06:00:00</th>\n",
              "      <th>07:00:00</th>\n",
              "      <th>08:00:00</th>\n",
              "      <th>09:00:00</th>\n",
              "      <th>10:00:00</th>\n",
              "      <th>11:00:00</th>\n",
              "      <th>12:00:00</th>\n",
              "      <th>13:00:00</th>\n",
              "      <th>14:00:00</th>\n",
              "      <th>15:00:00</th>\n",
              "      <th>16:00:00</th>\n",
              "      <th>17:00:00</th>\n",
              "      <th>18:00:00</th>\n",
              "      <th>19:00:00</th>\n",
              "      <th>20:00:00</th>\n",
              "      <th>21:00:00</th>\n",
              "      <th>22:00:00</th>\n",
              "      <th>23:00:00</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2015-01-01</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>65.0000</td>\n",
              "      <td>65.0000</td>\n",
              "      <td>65.000</td>\n",
              "      <td>65.0000</td>\n",
              "      <td>65.0000</td>\n",
              "      <td>65.7500</td>\n",
              "      <td>66.0000</td>\n",
              "      <td>66.00</td>\n",
              "      <td>66.00</td>\n",
              "      <td>66.2500</td>\n",
              "      <td>67.0000</td>\n",
              "      <td>67.0000</td>\n",
              "      <td>67.0000</td>\n",
              "      <td>67.00</td>\n",
              "      <td>67.00</td>\n",
              "      <td>67.0000</td>\n",
              "      <td>67.000</td>\n",
              "      <td>67.0000</td>\n",
              "      <td>67.0000</td>\n",
              "      <td>66.2500</td>\n",
              "      <td>66.00</td>\n",
              "      <td>66.7500</td>\n",
              "      <td>67.0000</td>\n",
              "      <td>67.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-02</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>155.7500</td>\n",
              "      <td>154.0000</td>\n",
              "      <td>154.500</td>\n",
              "      <td>156.2500</td>\n",
              "      <td>145.5000</td>\n",
              "      <td>149.2500</td>\n",
              "      <td>151.5000</td>\n",
              "      <td>153.25</td>\n",
              "      <td>154.75</td>\n",
              "      <td>155.5000</td>\n",
              "      <td>156.5000</td>\n",
              "      <td>157.0000</td>\n",
              "      <td>159.0000</td>\n",
              "      <td>159.00</td>\n",
              "      <td>159.00</td>\n",
              "      <td>153.5000</td>\n",
              "      <td>154.750</td>\n",
              "      <td>156.0000</td>\n",
              "      <td>156.7500</td>\n",
              "      <td>158.0000</td>\n",
              "      <td>158.00</td>\n",
              "      <td>158.0000</td>\n",
              "      <td>159.0000</td>\n",
              "      <td>158.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-03</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>159.0000</td>\n",
              "      <td>159.0000</td>\n",
              "      <td>159.750</td>\n",
              "      <td>159.2500</td>\n",
              "      <td>159.0000</td>\n",
              "      <td>157.5000</td>\n",
              "      <td>158.7500</td>\n",
              "      <td>159.00</td>\n",
              "      <td>159.25</td>\n",
              "      <td>159.0000</td>\n",
              "      <td>159.0000</td>\n",
              "      <td>158.0000</td>\n",
              "      <td>157.5000</td>\n",
              "      <td>156.00</td>\n",
              "      <td>154.75</td>\n",
              "      <td>151.7500</td>\n",
              "      <td>151.500</td>\n",
              "      <td>153.5000</td>\n",
              "      <td>156.2500</td>\n",
              "      <td>156.7500</td>\n",
              "      <td>156.00</td>\n",
              "      <td>155.0000</td>\n",
              "      <td>154.7500</td>\n",
              "      <td>153.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-04</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-05</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>137.9375</td>\n",
              "      <td>136.9375</td>\n",
              "      <td>135.625</td>\n",
              "      <td>133.4375</td>\n",
              "      <td>130.4375</td>\n",
              "      <td>123.3125</td>\n",
              "      <td>112.6875</td>\n",
              "      <td>101.00</td>\n",
              "      <td>89.50</td>\n",
              "      <td>80.5625</td>\n",
              "      <td>75.6875</td>\n",
              "      <td>72.9375</td>\n",
              "      <td>93.5625</td>\n",
              "      <td>86.50</td>\n",
              "      <td>74.00</td>\n",
              "      <td>59.4375</td>\n",
              "      <td>46.125</td>\n",
              "      <td>40.0625</td>\n",
              "      <td>40.6875</td>\n",
              "      <td>46.8125</td>\n",
              "      <td>55.25</td>\n",
              "      <td>61.9375</td>\n",
              "      <td>63.9375</td>\n",
              "      <td>57.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-02</th>\n",
              "      <td>-4.97</td>\n",
              "      <td>-10.10</td>\n",
              "      <td>-16.95</td>\n",
              "      <td>-11.70</td>\n",
              "      <td>-5.98</td>\n",
              "      <td>-5.21</td>\n",
              "      <td>-4.98</td>\n",
              "      <td>0.09</td>\n",
              "      <td>13.06</td>\n",
              "      <td>24.50</td>\n",
              "      <td>27.12</td>\n",
              "      <td>29.43</td>\n",
              "      <td>35.00</td>\n",
              "      <td>33.36</td>\n",
              "      <td>33.75</td>\n",
              "      <td>34.20</td>\n",
              "      <td>28.87</td>\n",
              "      <td>40.27</td>\n",
              "      <td>40.73</td>\n",
              "      <td>36.93</td>\n",
              "      <td>27.05</td>\n",
              "      <td>21.99</td>\n",
              "      <td>23.87</td>\n",
              "      <td>17.38</td>\n",
              "      <td>1391.5000</td>\n",
              "      <td>1343.5000</td>\n",
              "      <td>1293.000</td>\n",
              "      <td>1251.2500</td>\n",
              "      <td>1209.2500</td>\n",
              "      <td>1124.5000</td>\n",
              "      <td>1006.5000</td>\n",
              "      <td>874.75</td>\n",
              "      <td>739.50</td>\n",
              "      <td>619.2500</td>\n",
              "      <td>522.7500</td>\n",
              "      <td>465.5000</td>\n",
              "      <td>431.5000</td>\n",
              "      <td>408.00</td>\n",
              "      <td>390.50</td>\n",
              "      <td>368.7500</td>\n",
              "      <td>348.250</td>\n",
              "      <td>347.5000</td>\n",
              "      <td>364.2500</td>\n",
              "      <td>388.7500</td>\n",
              "      <td>433.00</td>\n",
              "      <td>496.0000</td>\n",
              "      <td>586.7500</td>\n",
              "      <td>726.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-03</th>\n",
              "      <td>15.92</td>\n",
              "      <td>15.55</td>\n",
              "      <td>14.38</td>\n",
              "      <td>9.32</td>\n",
              "      <td>13.26</td>\n",
              "      <td>14.03</td>\n",
              "      <td>27.06</td>\n",
              "      <td>36.49</td>\n",
              "      <td>39.97</td>\n",
              "      <td>39.26</td>\n",
              "      <td>35.52</td>\n",
              "      <td>32.97</td>\n",
              "      <td>29.08</td>\n",
              "      <td>28.38</td>\n",
              "      <td>28.73</td>\n",
              "      <td>28.29</td>\n",
              "      <td>33.02</td>\n",
              "      <td>37.95</td>\n",
              "      <td>37.99</td>\n",
              "      <td>36.57</td>\n",
              "      <td>31.00</td>\n",
              "      <td>27.16</td>\n",
              "      <td>27.13</td>\n",
              "      <td>24.76</td>\n",
              "      <td>716.7500</td>\n",
              "      <td>853.5000</td>\n",
              "      <td>972.000</td>\n",
              "      <td>1044.5000</td>\n",
              "      <td>1068.5000</td>\n",
              "      <td>1061.2500</td>\n",
              "      <td>1062.2500</td>\n",
              "      <td>1077.25</td>\n",
              "      <td>1091.00</td>\n",
              "      <td>1112.5000</td>\n",
              "      <td>1144.0000</td>\n",
              "      <td>1157.5000</td>\n",
              "      <td>1152.2500</td>\n",
              "      <td>1150.25</td>\n",
              "      <td>1150.25</td>\n",
              "      <td>1155.7500</td>\n",
              "      <td>1175.250</td>\n",
              "      <td>1220.0000</td>\n",
              "      <td>1273.5000</td>\n",
              "      <td>1314.0000</td>\n",
              "      <td>1345.00</td>\n",
              "      <td>1366.7500</td>\n",
              "      <td>1393.0000</td>\n",
              "      <td>1393.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-04</th>\n",
              "      <td>20.79</td>\n",
              "      <td>17.41</td>\n",
              "      <td>16.24</td>\n",
              "      <td>12.96</td>\n",
              "      <td>13.42</td>\n",
              "      <td>15.88</td>\n",
              "      <td>24.88</td>\n",
              "      <td>29.70</td>\n",
              "      <td>35.01</td>\n",
              "      <td>33.48</td>\n",
              "      <td>29.90</td>\n",
              "      <td>29.03</td>\n",
              "      <td>27.07</td>\n",
              "      <td>26.43</td>\n",
              "      <td>27.02</td>\n",
              "      <td>29.05</td>\n",
              "      <td>31.42</td>\n",
              "      <td>39.92</td>\n",
              "      <td>41.30</td>\n",
              "      <td>40.92</td>\n",
              "      <td>39.75</td>\n",
              "      <td>30.13</td>\n",
              "      <td>30.36</td>\n",
              "      <td>26.94</td>\n",
              "      <td>1415.5000</td>\n",
              "      <td>1417.5000</td>\n",
              "      <td>1417.750</td>\n",
              "      <td>1417.2500</td>\n",
              "      <td>1417.5000</td>\n",
              "      <td>1411.2500</td>\n",
              "      <td>1398.0000</td>\n",
              "      <td>1373.75</td>\n",
              "      <td>1333.50</td>\n",
              "      <td>1276.7500</td>\n",
              "      <td>1200.2500</td>\n",
              "      <td>1107.5000</td>\n",
              "      <td>1014.5000</td>\n",
              "      <td>983.75</td>\n",
              "      <td>1017.75</td>\n",
              "      <td>1089.7500</td>\n",
              "      <td>1156.750</td>\n",
              "      <td>1196.2500</td>\n",
              "      <td>1194.0000</td>\n",
              "      <td>1144.7500</td>\n",
              "      <td>1066.50</td>\n",
              "      <td>996.0000</td>\n",
              "      <td>916.0000</td>\n",
              "      <td>797.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-05</th>\n",
              "      <td>25.44</td>\n",
              "      <td>25.00</td>\n",
              "      <td>24.43</td>\n",
              "      <td>23.63</td>\n",
              "      <td>24.83</td>\n",
              "      <td>26.62</td>\n",
              "      <td>37.54</td>\n",
              "      <td>44.91</td>\n",
              "      <td>49.16</td>\n",
              "      <td>44.78</td>\n",
              "      <td>41.37</td>\n",
              "      <td>40.00</td>\n",
              "      <td>37.07</td>\n",
              "      <td>35.16</td>\n",
              "      <td>35.17</td>\n",
              "      <td>37.13</td>\n",
              "      <td>40.25</td>\n",
              "      <td>45.45</td>\n",
              "      <td>45.83</td>\n",
              "      <td>45.66</td>\n",
              "      <td>41.74</td>\n",
              "      <td>35.92</td>\n",
              "      <td>31.99</td>\n",
              "      <td>29.70</td>\n",
              "      <td>859.0000</td>\n",
              "      <td>729.7500</td>\n",
              "      <td>622.750</td>\n",
              "      <td>545.5000</td>\n",
              "      <td>493.5000</td>\n",
              "      <td>454.5000</td>\n",
              "      <td>428.7500</td>\n",
              "      <td>364.50</td>\n",
              "      <td>315.25</td>\n",
              "      <td>258.5000</td>\n",
              "      <td>226.2500</td>\n",
              "      <td>238.5000</td>\n",
              "      <td>274.2500</td>\n",
              "      <td>381.25</td>\n",
              "      <td>558.00</td>\n",
              "      <td>696.2500</td>\n",
              "      <td>804.750</td>\n",
              "      <td>880.5000</td>\n",
              "      <td>914.7500</td>\n",
              "      <td>996.7500</td>\n",
              "      <td>1031.50</td>\n",
              "      <td>1048.5000</td>\n",
              "      <td>1060.2500</td>\n",
              "      <td>1061.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-06</th>\n",
              "      <td>26.51</td>\n",
              "      <td>25.35</td>\n",
              "      <td>24.47</td>\n",
              "      <td>24.44</td>\n",
              "      <td>25.25</td>\n",
              "      <td>27.97</td>\n",
              "      <td>32.96</td>\n",
              "      <td>43.15</td>\n",
              "      <td>44.88</td>\n",
              "      <td>42.90</td>\n",
              "      <td>42.19</td>\n",
              "      <td>40.81</td>\n",
              "      <td>39.25</td>\n",
              "      <td>38.01</td>\n",
              "      <td>37.94</td>\n",
              "      <td>38.06</td>\n",
              "      <td>39.09</td>\n",
              "      <td>45.95</td>\n",
              "      <td>46.83</td>\n",
              "      <td>47.21</td>\n",
              "      <td>43.63</td>\n",
              "      <td>37.98</td>\n",
              "      <td>37.66</td>\n",
              "      <td>33.58</td>\n",
              "      <td>1049.2500</td>\n",
              "      <td>1029.2500</td>\n",
              "      <td>1001.250</td>\n",
              "      <td>978.5000</td>\n",
              "      <td>964.0000</td>\n",
              "      <td>921.5000</td>\n",
              "      <td>852.7500</td>\n",
              "      <td>787.00</td>\n",
              "      <td>727.50</td>\n",
              "      <td>680.5000</td>\n",
              "      <td>639.7500</td>\n",
              "      <td>613.5000</td>\n",
              "      <td>595.5000</td>\n",
              "      <td>581.75</td>\n",
              "      <td>572.50</td>\n",
              "      <td>557.0000</td>\n",
              "      <td>532.750</td>\n",
              "      <td>496.2500</td>\n",
              "      <td>455.5000</td>\n",
              "      <td>417.0000</td>\n",
              "      <td>367.75</td>\n",
              "      <td>316.2500</td>\n",
              "      <td>260.5000</td>\n",
              "      <td>201.75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1863 rows Ã— 48 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Day Ahead Price                    ... Wind offshore Generation                    \n",
              "                  00:00:00 01:00:00 02:00:00  ...                 21:00:00   22:00:00 23:00:00\n",
              "2015-01-01             NaN      NaN      NaN  ...                  66.7500    67.0000    67.00\n",
              "2015-01-02             NaN      NaN      NaN  ...                 158.0000   159.0000   158.25\n",
              "2015-01-03             NaN      NaN      NaN  ...                 155.0000   154.7500   153.75\n",
              "2015-01-04             NaN      NaN      NaN  ...                      NaN        NaN      NaN\n",
              "2015-01-05             NaN      NaN      NaN  ...                  61.9375    63.9375    57.75\n",
              "...                    ...      ...      ...  ...                      ...        ...      ...\n",
              "2020-02-02           -4.97   -10.10   -16.95  ...                 496.0000   586.7500   726.25\n",
              "2020-02-03           15.92    15.55    14.38  ...                1366.7500  1393.0000  1393.50\n",
              "2020-02-04           20.79    17.41    16.24  ...                 996.0000   916.0000   797.25\n",
              "2020-02-05           25.44    25.00    24.43  ...                1048.5000  1060.2500  1061.75\n",
              "2020-02-06           26.51    25.35    24.47  ...                 316.2500   260.5000   201.75\n",
              "\n",
              "[1863 rows x 48 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7NuEz3EKOTV",
        "colab_type": "text"
      },
      "source": [
        "**Windowed dataset using Keras Timeseries generator**\n",
        "\n",
        "Most models will work not on the entire time-series, but on a windowed subset of the time series. Keras has an inbuilt windowed dataset generator, but it helps to know a few things before we use that. It is primarily meant for use with Keras, so it produces a generator that outputs the dataset in batches as a list $[b_1, b_2, ...]$. Each batch $b_i$ consists of $[X,y]$, with each being a numpy array. The format here is as follows: each $X$ is a windowed dataset with following indices: `(sample_num, timestep, feature)`, while $y$ is of the form `(sample_num, value)`. The assumption here is that `y` consists of a single timestep, which works fine in our case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTIeAIoDKOw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "\n",
        "def get_windowed_dataset(df, X_cols, y_cols, window_size, return_generator=False, **kwargs):\n",
        "  X_data = extractHourlyData(df, X_cols)\n",
        "  y_data = extractHourlyData(df, y_cols)\n",
        "  # use batch size if defined, else return the whole dataset in one batch\n",
        "  batch_size = kwargs['batch_size'] or len(X_data)\n",
        "  generator = TimeseriesGenerator(\n",
        "    X_data, \n",
        "    y_data, \n",
        "    length = window_size, \n",
        "    sampling_rate = 1,\n",
        "    batch_size = batch_size)\n",
        "  if return_generator:\n",
        "    return generator\n",
        "  else:\n",
        "    X_batches = [X for (X, y) in generator]\n",
        "    y_batches = [y for (X, y) in generator]\n",
        "    return X_batches, y_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrUDKYQGTceQ",
        "colab_type": "text"
      },
      "source": [
        "Just a quick check to see if the generator works as expected:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp4U3qYrTbzw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "outputId": "3e490e7c-6516-4544-9079-b22ae241022a"
      },
      "source": [
        "generator = get_windowed_dataset(\n",
        "    df, \n",
        "    ['Day Ahead Price'], \n",
        "    ['Day Ahead Price'], \n",
        "    1, \n",
        "    return_generator=True, \n",
        "    batch_size=len(df)\n",
        ")\n",
        "\n",
        "generator[0][0].shape, generator[0][1].shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Date'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f95a90bdc405>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mreturn_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-0f6af215ca4d>\u001b[0m in \u001b[0;36mget_windowed_dataset\u001b[0;34m(df, X_cols, y_cols, window_size, return_generator, **kwargs)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_windowed_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mX_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractHourlyData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0my_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractHourlyData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# use batch size if defined, else return the whole dataset in one batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2ba691a77a39>\u001b[0m in \u001b[0;36mextractHourlyData\u001b[0;34m(df, data_columns, hour_column, date_column)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextractHourlyData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhour_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Hour'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mnum_of_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mnum_of_data_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mhourly_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_of_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_of_data_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_column\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Date'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9NgiOPfTfxL",
        "colab_type": "text"
      },
      "source": [
        "Before proceeding to investigate our models, it is important to figure out a cross-validation strategy and testing strategy using a metric to assess the performance of each of our models. So, let us briefly touch upon these topics first.  \n",
        "\n",
        "1. The cross-validation strategy for temporal data is a bit different from randomly choosing a subset of the dataset. Since the goal of our analysis is to forecast future values using present data, we will likewise train on a dataset from the past, and validate it against a subset to the future of it. This simulates the actual situation at hand; see [3, 4] for more details.\n",
        "2. For a simple criterion to do a quick comparison of algorithms as a first step in model evaluation, the sMAPE suffices:\n",
        "$$s = \\frac{|y-\\hat{y}|}{2(|y| + |\\hat{y}|)} $$ This should be sufficient for a first-order estimate of how well our models are doing; later on, we can perform more sophisticated forms of tests for our analysis.\n",
        "\n",
        "To cover pt. 1, we create a function to extract dataset into a train and test form chronologically separated as described. We also create a GridsearchCV equivalent for time series for hyperparameter search in ML models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF5vGQvcUKmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_train_timesplit(df, date_col_name='Date', train_size=0.9, test_size=0.1):\n",
        "  \"\"\"\n",
        "    Returns test-train split data based on date with test chronologically later than train data.\n",
        "  \"\"\"\n",
        "  min_date = df[date_col_name].min()\n",
        "  max_date = df[date_col_name].max()\n",
        "  train_split_date = min_date + (train_size*(max_date - min_date))\n",
        "  test_split_date = train_split_date + (test_size*(max_date - min_date))\n",
        "  train_df = df[df[date_col_name] < train_split_date]\n",
        "  test_df = df[(df[date_col_name] > train_split_date) & (df[date_col_name] < test_split_date)]\n",
        "  return train_df, test_df\n",
        "\n",
        "def day_forward_chaining(df, date_col_name='Date', k=10):\n",
        "  for i in range(1,k):\n",
        "    yield test_train_timesplit(df, date_col_name, train_size=(i/k), test_size=(1/k))\n",
        "\n",
        "# The above function is the time-series equivalent of gridsearch CV\n",
        "# Example use-case:\n",
        "# for train_df, test_df in day_forward_chaining(german_df):\n",
        "#   <<< do your model training and tuning here >>>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YxvlQ3lry9F",
        "colab_type": "text"
      },
      "source": [
        "For the second part, to compute sMAPE for model evaluation, the following class is defined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHq0dSInr4zw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class error_analysis:\n",
        "  def __init__(self, predictor):\n",
        "    self.predictor = predictor\n",
        "  def smape_by_entry_from_generator(self, generator):\n",
        "    smape_error = {}\n",
        "    yhat, y = np.zeros((0,24)), np.zeros((0,24))\n",
        "    for X_i, y_i in generator:\n",
        "      yhat_i = self.predictor(X_i)\n",
        "      assert yhat_i.shape == y_i.shape\n",
        "      # add to our list\n",
        "      yhat, y = np.concatenate([yhat_i, yhat], axis=0), np.concatenate([y_i, y], axis=0)\n",
        "    for h in range(24):\n",
        "      smape_error[h] = np.absolute(yhat[:,h] - y[:,h])/(np.absolute(yhat[:,h]) + np.absolute(y[:,h]))\n",
        "    return pd.DataFrame(smape_error)\n",
        "  def smape_by_entry_from_dataset(self, X, y):\n",
        "    smape_error = {}\n",
        "    yhat, y = np.zeros((0,24)), np.zeros((0,24))\n",
        "    for X_i, y_i in generator:\n",
        "      yhat_i = self.predictor(X_i)\n",
        "      assert yhat_i.shape == y_i.shape\n",
        "      # add to our list\n",
        "      yhat, y = np.concatenate([yhat_i, yhat], axis=0), np.concatenate([y_i, y], axis=0)\n",
        "    for h in range(24):\n",
        "      smape_error[h] = np.absolute(yhat[:,h] - y[:,h])/(np.absolute(yhat[:,h]) + np.absolute(y[:,h]))\n",
        "    return pd.DataFrame(smape_error)    \n",
        "  def smape_by_hour(self, generator):\n",
        "    error_df = self.smape_by_entry_from_generator(generator)\n",
        "    error_df.replace(np.inf, 0) # remove infinite values\n",
        "    return error_df.mean(axis=0, skipna=True)*200\n",
        "  def total_smape(self, generator):\n",
        "    smape_by_hour = self.smape_by_hour(generator)\n",
        "    return smape_by_hour.mean(axis=0, skipna=True)\n",
        "  def plot(self, generator):\n",
        "    plt.figure(figsize=(15,8))\n",
        "    error_df = self.smape_by_entry(generator)\n",
        "    error_df.boxplot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G_afRkMM-Cl",
        "colab_type": "text"
      },
      "source": [
        "# Naive model and the baseline\n",
        "\n",
        "The simplest model one can think of is as follows: $p_{t+h} = p_t $ for all $h$ in the future window. This should allow us to setup a baseline, which any sensible model should perform better than. This is primarily helpful in debugging complex models when making sure that we are not doing anything terribly stupid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CAQg6j9NmN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def naivemodel(X):\n",
        "  return X[:,0,:]\n",
        "\n",
        "# Since this model needs no training, \n",
        "# we can just test it on the whole dataset\n",
        "X, y = get_windowed_dataset(german_df, ['Day Ahead Price'], ['Day Ahead Price'], 1)\n",
        "\n",
        "def val_generator(): \n",
        "  yield [X, y]\n",
        "\n",
        "naive_error = error_analysis(naivemodel)\n",
        "naive_error.total_smape(val_generator())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8lg9F7UsI5D",
        "colab_type": "text"
      },
      "source": [
        "So, there's our baseline. Anything that performs worse usually means that we are doing something wrong.  \n",
        "\n",
        "Another simple baseline is to simply take the weighted average of the previous values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEAwcicoCa35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mamodel(X, alpha=0.1):\n",
        "  window_size = X.shape[1]\n",
        "  weight = np.array([pow(alpha, i) for i in reversed(range(window_size))])\n",
        "  return np.tensordot(weight, X, axes=(0,1)) / np.sum(weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxKxylBJN9mc",
        "colab_type": "text"
      },
      "source": [
        "# Regression\n",
        "\n",
        "For time-series data, regression usually presents itself in the form of auto-regression, i.e. regression in terms of previous time-step variable. A very simple example would be the $AR(k)$ models, where the hypothesis is as follows:\n",
        "$$p_{t+h}=\\sum_{i=0}\\alpha_ip_{t-i}+\\epsilon$$ where $\\epsilon$ is random white noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUYzrgu_N_aI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz8K_FDnWRyu",
        "colab_type": "text"
      },
      "source": [
        "The above technique is a very simple one; there are far more sophistical statistical techniques available. We will pursue them here through the use of `statsmodels` library. I will not explore the algorithms themselves in detail (though it will make for a good series of blog posts sometime in the near future; for now refer to [1] for details)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq8bl3eYi4cx",
        "colab_type": "text"
      },
      "source": [
        "## Univariate AR(k) with time trend\n",
        "\n",
        "A simple way to approach the problem is to consider each hourly day-ahead price as a separate time-series. This lets us use the AR(k) and ARMA(p,q) models on them directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIfxAmgJiq4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statsmodels.tsa.ar_model import AutoReg, ar_select_order\n",
        "from statsmodels.tsa.api import acf, pacf, graphics\n",
        "\n",
        "AR_model = AutoReg(german_df, 3, trend='t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeX2O_KrCRsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.set_style('darkgrid')\n",
        "sns.mpl.rc('figure',figsize=(16, 6))\n",
        "res.plot_predict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVQoWhaZn6n_",
        "colab_type": "text"
      },
      "source": [
        "## Multivariate: VAR(k) and VECM\n",
        "\n",
        "Instead of treating our series as a univariate time series for each hour, one can expect an improvement by considering it proper as a multivariate time series analysis. The relevant algorithms are the vector autoregressive model (VAR) and the vector error correction model (VECM)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U4VJOsxO6Nw",
        "colab_type": "text"
      },
      "source": [
        "# Exponential Smoothing and Holtz-Winter models\n",
        "\n",
        "Another popular approach to time-series problems are the ETS class of techniques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v65p83Wno7Df",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statsmodels.tsa.api import SimpleExpSmoothing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbcplWTMCD7r",
        "colab_type": "text"
      },
      "source": [
        "$$y_t = y_{t-1} + \\alpha (y_{t-1} - \\hat{y}_{t-1})$$\n",
        "\n",
        "$$y_{t+h} = l_t +h b_t + s_{t+h-m(k+1)}$$\n",
        "$$l_t = \\alpha \\frac{y_t}{s_{t-m}} + (1-\\alpha)(l_{t-1} + b_{t-1})$$\n",
        "$$b_t = \\beta^*(l_t - l_{t-1}) + (1-\\beta^*)b_{t-1}$$\n",
        "$$s_t = \\gamma \\frac{y_t}{(l_{t-1} + b_{t-1})} + (1-\\gamma)s_{t-m}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWw8aLSsO9b6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statsmodels.tsa.api import ExponentialSmoothing, Holt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB4twMe_WHm0",
        "colab_type": "text"
      },
      "source": [
        "# Markov switching regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLDQ3w4wWNXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woxv6ti6kMbM",
        "colab_type": "text"
      },
      "source": [
        "# Bibliography\n",
        "\n",
        "**[HA18]** R. J. Hyndman, and G. Athanasopoulos, Forecasting: Principles and Practice, https://otexts.com/fpp2/index.html \n",
        "\n",
        "**[HWL15]**. R. J. Hyndman, E. Wang, N. Laptev, *Large-Scale Unusual Time Series Detection*,  [https://robjhyndman.com/papers/icdm2015.pdf]\n",
        "\n",
        "**[SE11]** More on the different approaches to the validation strategies for a time series here: https://stats.stackexchange.com/questions/14099/using-k-fold-cross-validation-for-time-series-model-selection\n",
        "\n",
        "**[Tas00]** L. J. Tashman, *Out-of-sample tests of forecasting accuracy: an analysis and review*, International Journal of Forecasting, 2000, vol. 16, issue 4, 437-450 [https://econpapers.repec.org/article/eeeintfor/v_3a16_3ay_3a2000_3ai_3a4_3ap_3a437-450.htm]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m1CXDz1x7GX",
        "colab_type": "text"
      },
      "source": [
        "Supplements:  \n",
        "[Using Facebook's forecast engine: Prophet]()  \n",
        "[GluonTS]()\n",
        "\n",
        "Next part: [Deep Models]()"
      ]
    }
  ]
}